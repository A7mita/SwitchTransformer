{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A7mita/SwitchTransformer/blob/main/Switch_Transformer_Layer_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# A simple Feed-Forward Network which will be our \"expert\"\n",
        "class Expert(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple feed-forward network used as an expert in the Switch Transformer.\n",
        "    It consists of two linear layers with a ReLU activation in between.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.relu(self.fc1(x)))\n",
        "\n",
        "class SwitchFeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    The Switch Transformer layer.\n",
        "\n",
        "    This layer implements the core logic of routing tokens to different experts.\n",
        "    It includes the router, the experts themselves, and the load balancing loss calculation.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_ff, num_experts, capacity_factor=1.25, drop_tokens=True):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "        self.num_experts = num_experts\n",
        "        self.capacity_factor = capacity_factor\n",
        "        self.drop_tokens = drop_tokens\n",
        "\n",
        "        # The router weights, mapping token embeddings to expert logits\n",
        "        self.router = nn.Linear(d_model, num_experts)\n",
        "        # A list of expert networks\n",
        "        self.experts = nn.ModuleList([Expert(d_model, d_ff) for _ in range(num_experts)])\n",
        "\n",
        "    def load_balancing_loss(self, router_probs, expert_mask):\n",
        "        \"\"\"\n",
        "        Calculates the load balancing loss as described in the Switch Transformer paper.\n",
        "\n",
        "        This loss encourages the router to send an equal number of tokens to each expert.\n",
        "\n",
        "        Args:\n",
        "            router_probs (torch.Tensor): Probabilities for each token-expert pair.\n",
        "                                         Shape: (num_tokens, num_experts)\n",
        "            expert_mask (torch.Tensor): A one-hot mask indicating which expert each token is routed to.\n",
        "                                        Shape: (num_tokens, num_experts)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The calculated load balancing loss.\n",
        "        \"\"\"\n",
        "        num_tokens = router_probs.shape[0]\n",
        "\n",
        "        # Fraction of tokens dispatched to each expert\n",
        "        tokens_per_expert = torch.sum(expert_mask, dim=0)\n",
        "        f_i = tokens_per_expert / num_tokens\n",
        "\n",
        "        # Fraction of router probability allocated to each expert\n",
        "        router_prob_per_expert = torch.sum(router_probs, dim=0)\n",
        "        P_i = router_prob_per_expert / num_tokens\n",
        "\n",
        "        # The loss is the scaled dot-product of f_i and P_i\n",
        "        loss = self.num_experts * torch.sum(f_i * P_i)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for the Switch Transformer layer.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor. Shape: (batch_size, seq_len, d_model)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output tensor from the layer. Shape: (batch_size, seq_len, d_model)\n",
        "            torch.Tensor: The auxiliary load balancing loss.\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "        # Flatten the input to treat all tokens independently\n",
        "        x = x.view(-1, d_model)\n",
        "        num_tokens = x.shape[0]\n",
        "\n",
        "        print(\"--- Input ---\")\n",
        "        print(f\"Original input shape: ({batch_size}, {seq_len}, {d_model})\")\n",
        "        print(f\"Reshaped input for routing (num_tokens, d_model): {x.shape}\\n\")\n",
        "\n",
        "        # --- 1. Routing Logic ---\n",
        "        # Get router logits and probabilities\n",
        "        router_logits = self.router(x)\n",
        "        router_probs = F.softmax(router_logits, dim=1)\n",
        "\n",
        "        # Get the top-1 expert index and the corresponding gate value (probability)\n",
        "        expert_gates, expert_indices = torch.max(router_probs, dim=1)\n",
        "\n",
        "        # Create a one-hot mask for the chosen experts\n",
        "        expert_mask = F.one_hot(expert_indices, self.num_experts).float()\n",
        "\n",
        "        print(\"--- Routing ---\")\n",
        "        print(f\"Router logits shape: {router_logits.shape}\")\n",
        "        print(f\"Router probabilities shape: {router_probs.shape}\")\n",
        "        print(f\"Expert indices (top-1 choice for each token): {expert_indices.shape}\\n{expert_indices}\\n\")\n",
        "        print(f\"Expert gates (probability of top-1 choice): {expert_gates.shape}\\n\")\n",
        "        print(f\"Expert mask (one-hot): {expert_mask.shape}\\n\")\n",
        "\n",
        "        # --- 2. Load Balancing Loss ---\n",
        "        aux_loss = self.load_balancing_loss(router_probs, expert_mask)\n",
        "        print(\"--- Load Balancing ---\")\n",
        "        print(f\"Auxiliary Load Balancing Loss: {aux_loss.item():.4f}\\n\")\n",
        "\n",
        "\n",
        "        # --- 3. Capacity Calculation and Token Dropping ---\n",
        "        # Each expert has a fixed capacity\n",
        "        expert_capacity = math.ceil((num_tokens / self.num_experts) * self.capacity_factor)\n",
        "\n",
        "        # Count how many tokens are assigned to each expert\n",
        "        tokens_per_expert = torch.sum(expert_mask, dim=0)\n",
        "\n",
        "        print(\"--- Capacity ---\")\n",
        "        print(f\"Expert capacity (max tokens per expert): {expert_capacity}\")\n",
        "        print(f\"Tokens assigned to each expert: {tokens_per_expert.cpu().numpy()}\\n\")\n",
        "\n",
        "        # Get the position of each token within its expert's batch\n",
        "        # This is used to drop tokens if an expert's capacity is exceeded\n",
        "        position_in_expert = torch.cumsum(expert_mask, dim=0) * expert_mask\n",
        "\n",
        "        # Keep only tokens that are within the expert's capacity\n",
        "        within_capacity = position_in_expert <= expert_capacity\n",
        "\n",
        "        if self.drop_tokens:\n",
        "            expert_mask = expert_mask * within_capacity.float()\n",
        "\n",
        "        # The final gate value is the original gate masked by the capacity\n",
        "        final_expert_gates = expert_gates * expert_mask.sum(dim=1)\n",
        "\n",
        "        print(\"--- Token Dispatching (Pre-computation) ---\")\n",
        "        print(f\"Position of each token in its expert's queue: {position_in_expert.shape}\")\n",
        "        print(f\"Final expert mask (after capacity check): {expert_mask.shape}\")\n",
        "        dropped_tokens = num_tokens - expert_mask.sum()\n",
        "        print(f\"Tokens dropped due to capacity overflow: {dropped_tokens.item()}\\n\")\n",
        "\n",
        "        # --- 4. Dispatch Tokens to Experts and Combine Results ---\n",
        "        # Create a final output tensor of zeros\n",
        "        y = torch.zeros_like(x)\n",
        "\n",
        "        # This loop iterates through each expert, processes the tokens routed to it,\n",
        "        # and scatters the results back to the correct positions in the output tensor.\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            # Find which tokens are routed to this expert\n",
        "            token_indices = (expert_mask[:, i] == 1).nonzero(as_tuple=True)[0]\n",
        "\n",
        "            if token_indices.numel() > 0:\n",
        "                # Gather the tokens for this expert\n",
        "                expert_input = x[token_indices]\n",
        "\n",
        "                # Process tokens through the expert\n",
        "                expert_output = expert(expert_input)\n",
        "\n",
        "                # Scatter the expert's output back to the main tensor `y`\n",
        "                y.index_add_(0, token_indices, expert_output)\n",
        "\n",
        "                print(f\"-> Expert {i}: Processed {token_indices.numel()} tokens.\")\n",
        "\n",
        "        # Scale the output by the final gate values\n",
        "        y = y * final_expert_gates.unsqueeze(1)\n",
        "\n",
        "        # Reshape the output back to the original input shape\n",
        "        y = y.view(batch_size, seq_len, d_model)\n",
        "\n",
        "        print(\"\\n--- Output ---\")\n",
        "        print(f\"Final output shape (after combining experts): {y.shape}\")\n",
        "\n",
        "        return y, aux_loss\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Toy Configuration ---\n",
        "    # Using small values to make the output easy to inspect\n",
        "    batch_size = 2\n",
        "    seq_len = 5\n",
        "    d_model = 8  # Hidden dimension size\n",
        "    d_ff = 16    # Feed-forward inner dimension\n",
        "    num_experts = 4\n",
        "    capacity_factor = 1.0 # Set to 1.0 for simplicity, can be > 1.0\n",
        "\n",
        "    print(\"=\"*50)\n",
        "    print(\"Switch Transformer Layer - Toy Example\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Configuration: batch_size={batch_size}, seq_len={seq_len}, d_model={d_model}, \"\n",
        "          f\"num_experts={num_experts}, capacity_factor={capacity_factor}\\n\")\n",
        "\n",
        "    # --- Model and Data ---\n",
        "    # Instantiate the Switch layer\n",
        "    switch_ffn = SwitchFeedForward(d_model, d_ff, num_experts, capacity_factor)\n",
        "\n",
        "    # Create some random toy data\n",
        "    input_tensor = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "    # --- Run the Forward Pass ---\n",
        "    # Get the output and the auxiliary loss from the layer\n",
        "    output_tensor, loss = switch_ffn(input_tensor)\n",
        "\n",
        "    print(\"\\n--- Final Results ---\")\n",
        "    print(f\"Final Output Tensor Shape: {output_tensor.shape}\")\n",
        "    print(f\"Final Auxiliary Loss: {loss.item():.4f}\")\n",
        "    print(\"=\"*50)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Switch Transformer Layer - Toy Example\n",
            "==================================================\n",
            "Configuration: batch_size=2, seq_len=5, d_model=8, num_experts=4, capacity_factor=1.0\n",
            "\n",
            "--- Input ---\n",
            "Original input shape: (2, 5, 8)\n",
            "Reshaped input for routing (num_tokens, d_model): torch.Size([10, 8])\n",
            "\n",
            "--- Routing ---\n",
            "Router logits shape: torch.Size([10, 4])\n",
            "Router probabilities shape: torch.Size([10, 4])\n",
            "Expert indices (top-1 choice for each token): torch.Size([10])\n",
            "tensor([1, 0, 1, 3, 1, 0, 0, 1, 1, 1])\n",
            "\n",
            "Expert gates (probability of top-1 choice): torch.Size([10])\n",
            "\n",
            "Expert mask (one-hot): torch.Size([10, 4])\n",
            "\n",
            "--- Load Balancing ---\n",
            "Auxiliary Load Balancing Loss: 1.1515\n",
            "\n",
            "--- Capacity ---\n",
            "Expert capacity (max tokens per expert): 3\n",
            "Tokens assigned to each expert: [3. 6. 0. 1.]\n",
            "\n",
            "--- Token Dispatching (Pre-computation) ---\n",
            "Position of each token in its expert's queue: torch.Size([10, 4])\n",
            "Final expert mask (after capacity check): torch.Size([10, 4])\n",
            "Tokens dropped due to capacity overflow: 3.0\n",
            "\n",
            "-> Expert 0: Processed 3 tokens.\n",
            "-> Expert 1: Processed 3 tokens.\n",
            "-> Expert 3: Processed 1 tokens.\n",
            "\n",
            "--- Output ---\n",
            "Final output shape (after combining experts): torch.Size([2, 5, 8])\n",
            "\n",
            "--- Final Results ---\n",
            "Final Output Tensor Shape: torch.Size([2, 5, 8])\n",
            "Final Auxiliary Loss: 1.1515\n",
            "==================================================\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNZvlMuKJWUf",
        "outputId": "a5e608f1-28ae-4e10-bb61-6f9e7e28ef1a"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}